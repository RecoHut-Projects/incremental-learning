{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKyhYsZdPGZI"
      },
      "source": [
        "# RKMF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phmFjzdKPMef"
      },
      "source": [
        "A kernel function allows to transform the product of the factor matrices. Kernels like the s-shaped logistic function allow to impose bounds on the prediction (e.g. one to five stars) while still being differentiable.\n",
        "\n",
        "The matrix factorization can be expressed as:\n",
        "\n",
        "$$\\hat{r}_{ui} = b_{u,i} + \\sum_{f=1}^kw_{u,f}h_{i,f}$$\n",
        "\n",
        "Like matrix factorization, kernel matrix factorization (KMF) uses two feature matrices that contain the features for users and items, respectively. But the interactions between the feature vector $w_u$ of a user and the feature vector $h_i$ of an item are kernelized:\n",
        "\n",
        "$$\\hat{r}_{ui} = a + c\\ \\cdot \\ K(w_u,h_i)$$\n",
        "\n",
        "The terms $a$ and $c$ are introduced to allow re-scaling the predictions. For the kernel $K : \\mathbb{R}^k × \\mathbb{R}^k → \\mathbb{R}$ one can use any of the well-known kernels like linear, polynomial, RBF, logistic etc.\n",
        "\n",
        "It is obvious that normal matrix factorization can be expressed with $a = b_{u,i}$ and $c = 1$ and the linear kernel $K_l$.\n",
        "\n",
        "## Training procedure\n",
        "\n",
        "<p><center><img src='_images/L766388_1.png'></center></p>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "L766388_RKMF.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
